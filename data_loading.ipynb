{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ff5fa67",
   "metadata": {},
   "source": [
    "# Embedding of Loaded data and Pushing to MongoDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8e31de",
   "metadata": {},
   "source": [
    "### In this notebook we will be loading the data from csv format and creating Vector embedding using HF and pushing to MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f307f6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q langchain langchain-mongodb langchain-chroma langchain-cli langchain-community langchain-core langchain-huggingface langchain-text-splitters jq pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98e6114",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3921e9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting with importing the libraries\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.document_loaders import JSONLoader, PyPDFDirectoryLoader\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from langchain.docstore.document import Document\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880ee369",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\AKSHAT SHAW\\OneDrive - iitr.ac.in\\Desktop\\Side-Projects\\Zomato_RAG\\data\\restaurant_menus.csv\")\n",
    "\n",
    "docs = []\n",
    "for _, r in df.iterrows():\n",
    "    text = (\n",
    "        f\"Restaurant: {r['restaurant_name']}\\n\"\n",
    "        f\"Rating:     {r['rating']}\\n\"\n",
    "        f\"Price one:  {r['price_for_one']}\\n\"\n",
    "        f\"Cuisines:   {r['cuisine']}\\n\"\n",
    "        f\"Item:       {r['item_name']}\\n\"\n",
    "        f\"Description:{r['description'] if pd.notna(r['description']) else 'No description'}\\n\"\n",
    "        f\"Price:      {r['price']}\"\n",
    "    )\n",
    "    docs.append(Document(page_content=text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6a553b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "# Load the embedding model (https://huggingface.co/nomic-ai/nomic-embed-text-v1\")\n",
    "model = SentenceTransformer(\"nomic-ai/nomic-embed-text-v1\", trust_remote_code=True)\n",
    "def get_embedding(data):\n",
    "    \"\"\"Generates vector embeddings for the given data.\"\"\"\n",
    "    embedding = model.encode(data)\n",
    "    return embedding.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2e3d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "# Connect to your Atlas cluster\n",
    "uri = os.getenv(\"MONGODB_URI\")\n",
    "client = MongoClient(uri)\n",
    "collection = client[\"rag_db\"][\"test\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cc41da",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_to_insert = [{\n",
    "    \"text\": doc.page_content,\n",
    "    \"embedding\": get_embedding(doc.page_content)\n",
    "} for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba745807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert documents into the collection\n",
    "result = collection.insert_many(docs_to_insert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f1e6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo.operations import SearchIndexModel\n",
    "import time\n",
    "# Create index model, then create the search index\n",
    "index_name=\"vector_index\"\n",
    "search_index_model = SearchIndexModel(\n",
    "  definition = {\n",
    "    \"fields\": [\n",
    "      {\n",
    "        \"type\": \"vector\",\n",
    "        \"numDimensions\": 768,\n",
    "        \"path\": \"embedding\",\n",
    "        \"similarity\": \"cosine\"\n",
    "      }\n",
    "    ]\n",
    "  },\n",
    "  name = index_name,\n",
    "  type = \"vectorSearch\"\n",
    ")\n",
    "collection.create_search_index(model=search_index_model)\n",
    "# Wait for initial sync to complete\n",
    "print(\"Polling to check if the index is ready. This may take up to a minute.\")\n",
    "predicate=None\n",
    "if predicate is None:\n",
    "   predicate = lambda index: index.get(\"queryable\") is True\n",
    "while True:\n",
    "   indices = list(collection.list_search_indexes(index_name))\n",
    "   if len(indices) and predicate(indices[0]):\n",
    "      break\n",
    "   time.sleep(5)\n",
    "print(index_name + \" is ready for querying.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b09e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to run vector search queries\n",
    "def get_query_results(query):\n",
    "  \"\"\"Gets results from a vector search query.\"\"\"\n",
    "\n",
    "  query_embedding = get_embedding(query)\n",
    "  pipeline = [\n",
    "      {\n",
    "            \"$vectorSearch\": {\n",
    "              \"index\": \"vector_index\",\n",
    "              \"queryVector\": query_embedding,\n",
    "              \"path\": \"embedding\",\n",
    "              \"exact\": True,\n",
    "              \"limit\": 10\n",
    "            }\n",
    "      }, {\n",
    "            \"$project\": {\n",
    "              \"_id\": 0,\n",
    "              \"text\": 1\n",
    "         }\n",
    "      }\n",
    "  ]\n",
    "\n",
    "  results = collection.aggregate(pipeline)\n",
    "\n",
    "  array_of_results = []\n",
    "  for doc in results:\n",
    "      array_of_results.append(doc)\n",
    "  return array_of_results\n",
    "\n",
    "# Test the function with a sample query\n",
    "# import pprint\n",
    "# pprint.pprint(get_query_results(\"Chicken Biryani\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1830ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['HUGGINGFACEHUB_API_TOKEN']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56b2d9e",
   "metadata": {},
   "source": [
    "# Tesing responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b98ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import HuggingFaceHub\n",
    "\n",
    "hf=HuggingFaceHub(\n",
    "    repo_id=\"mistralai/Mixtral-8x7B-Instruct-v0.1\", #meta-llama/Llama-3.1-8B-Instruct\n",
    "    model_kwargs={\"temperature\":0.3}\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0efe15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(question, context):\n",
    "    system_prompt = (\n",
    "        \"You are a helpful assistant that answers user questions about restaurants using only the provided restaurant data.\\n\"\n",
    "        \"If the answer cannot be determined from the context, reply with 'I'm not sure based on the available information.'\\n\"\n",
    "        \"Always be concise, factual, and context-aware.\"\n",
    "\n",
    "    )\n",
    "\n",
    "    user_prompt = f\"\"\"Question: {question}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "    # Combine system and user messages\n",
    "    return f\"{system_prompt}\\n\\n{user_prompt}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e310aa3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Where can i find best fish?\"\n",
    "\n",
    "\n",
    "context_docs = get_query_results(question)\n",
    "\n",
    "#  Retrieve context\n",
    "# retrieved_docs = retriever.get_relevant_documents(question) #Based on Vector db not mongodb\n",
    "context = \"\\n\\n---\\n\\n\".join(text['text'] for text in context_docs)\n",
    "\n",
    "def format_docs(context_docs):\n",
    "  return \"\\n\\n---\\n\\n\".join(text['text'] for text in context_docs)\n",
    "\n",
    "input_prompt = build_prompt(question, context)\n",
    "\n",
    "#   Invoke LLM\n",
    "print(\"\\n\\n-----------------------------------------------------\")\n",
    "print(\"ðŸ§  AI Response:\\n\")\n",
    "response = hf.invoke(input_prompt)\n",
    "print(\"AI:\", response.split(\"Answer\")[1])\n",
    "print(\"\\n\\n-----------------------------------------------------\")\n",
    "print( response.split(\"Answer\")[0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
